{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dd4d36-609b-4e14-ba93-ebf08bdb15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Props to this sensei\n",
    "# https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a77eb7d-2891-46db-9649-70dbf52dd5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "          if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a483cfa-c20f-460b-b51c-7f2c8275853d",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89ca760-851f-4cfc-b0f4-f6837de852cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "text_file = \"tiny-shakespeare.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24e153-111f-490b-aa5f-9579bf99d022",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5123bdbe-4f29-476b-b2d3-2d142e5e7661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read file\n",
    "with open(text_file, \"r\") as f:\n",
    "    text = f.read()\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc14e1f1-3f5e-4e98-b838-cbbcb10b6585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the characters in the text: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Length of the characters: 65\n"
     ]
    }
   ],
   "source": [
    "char_list = sorted(list(set(text)))\n",
    "char_size = len(char_list)\n",
    "print(f\"All the characters in the text: {''.join(char_list)}\")\n",
    "print(f\"Length of the characters: {char_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe3542-8123-49f9-8ea7-2e070befb718",
   "metadata": {},
   "source": [
    "## Tokenizer (character based, index/ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7adea02c-1c7f-4b68-8013-2c5a5666eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        self.char_to_index = None\n",
    "        self.index_to_char = None\n",
    "\n",
    "    def fit(self, char_list):  \n",
    "        self.char_to_index = {char: idx for idx, char in enumerate(char_list)}\n",
    "        self.index_to_char = {idx: char for char, idx in self.char_to_index.items()}\n",
    "\n",
    "    def encode_index(self, input_str):\n",
    "        return [self.char_to_index[char] for char in input_str]\n",
    "\n",
    "    def decode_index(self, encoded_list):\n",
    "        return ''.join([self.index_to_char[idx] for idx in encoded_list])\n",
    "\n",
    "    @staticmethod\n",
    "    def ascii_tokenizer(char):\n",
    "        return ord(char)\n",
    "\n",
    "    @staticmethod\n",
    "    def ascii_decoder(ascii_value):\n",
    "        return chr(ascii_value)\n",
    "\n",
    "    def encode_combined(self, input_str, use_ascii=False):\n",
    "        if use_ascii:\n",
    "            return [self.ascii_tokenizer(char) for char in input_str]\n",
    "        else:\n",
    "            return self.encode_index(input_str)\n",
    "\n",
    "    def decode_combined(self, encoded_list, use_ascii=False):\n",
    "        if use_ascii:\n",
    "            return ''.join([self.ascii_decoder(ascii_value) for ascii_value in encoded_list])\n",
    "        else:\n",
    "            return self.decode_index(encoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c755ae-b836-4c4d-982c-201b91bc9c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String: Hello there\n",
      "Encoded List (ASCII): [72, 101, 108, 108, 111, 32, 116, 104, 101, 114, 101]\n",
      "Decoded String (ASCII): Hello there\n",
      "Encoded List (Index): [72, 101, 108, 108, 111, 32, 116, 104, 101, 114, 101]\n",
      "Decoded String (Index): Hello there\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "tokenizer = MyTokenizer()\n",
    "tokenizer.fit(char_list)\n",
    "\n",
    "input_str = \"Hello there\"\n",
    "encoded_list_ascii = tokenizer.encode_combined(input_str, use_ascii=True)\n",
    "decoded_str_ascii = tokenizer.decode_combined(encoded_list_ascii, use_ascii=True)\n",
    "\n",
    "encoded_list_index = tokenizer.encode_combined(input_str, use_ascii=True)\n",
    "decoded_str_index = tokenizer.decode_combined(encoded_list_index, use_ascii=True)\n",
    "\n",
    "print(\"Original String:\", input_str)\n",
    "print(\"Encoded List (ASCII):\", encoded_list_ascii)\n",
    "print(\"Decoded String (ASCII):\", decoded_str_ascii)\n",
    "\n",
    "print(\"Encoded List (Index):\", encoded_list_index)\n",
    "print(\"Decoded String (Index):\", decoded_str_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3773c490-c37e-453a-8057-9fa02db53bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode all the data \n",
    "encoded_data = tokenizer.encode_combined(text) \n",
    "encoded_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43202c3-4efd-44f0-99a8-8663cdbc3aa6",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d1d7c7-1c9c-4485-8bec-03ac33b217fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encoded_data)\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897f99c6-677a-44c7-8b16-e51881ab0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([61,  1, 28,  1, 21, 12, 24, 50,  1,  1,  1, 52, 44, 14,  1,  1, 58, 51,\n",
      "        33,  1,  1,  1, 58, 39, 50, 52, 43, 58, 40,  0,  1, 27,  1, 58,  1, 58,\n",
      "        43, 56, 56, 14, 46, 30,  6, 19, 43, 20, 42, 43,  6, 32,  0,  1, 10, 56,\n",
      "        53, 42, 56, 47, 47, 63, 53, 47, 43, 63])\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encoded_data):\n",
    "        self.encoded_data = encoded_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_data[idx]\n",
    "        \n",
    "dataset = MyDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# printing the first batch\n",
    "for batch in dataloader: \n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3534c75-0447-4a1f-98f1-e00bf209d784",
   "metadata": {},
   "source": [
    "## GPT and language models\n",
    "https://github.com/iVishalr/GPT/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a9954e-c3f3-46ed-9ed4-ef5b5716a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@mingzehe/implement-transformer-via-pytorch-step-by-step-part-2-69f020d580c6\n",
    "\n",
    "#attention \n",
    "def attention(k,q,v):\n",
    "    # q dim [batch_size,n_heads,length,d_tensor]\n",
    "    d_tensor = q.size(-1) \n",
    "    # assume dim of query/key/value vector should be same \n",
    "    # and it should be to make below calculation happen      \n",
    "    k_t = k.transpose(-2,-1) #[batch_size,n_heads,d_tensor,length]\n",
    "    score = (q @ k_t)/math.sqrt(d_tensor)\n",
    "    v= torch.softmax(score,dim=-1) @ v\n",
    "    return v,score\n",
    "\n",
    "import copy\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "  # reduced dim for each Q,K,V, but added up to d_model\n",
    "        self.d_k = d_model // n_head \n",
    "        self.n_head = n_head\n",
    "        self.attn = None\n",
    "  # use the attention class defined above\n",
    "        self.attention = attention() \n",
    "\n",
    "  # 3 for K,Q,V, the forth layer is on the top for final attention score\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4) \n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        samples = q.size(0) #q init as 512x512\n",
    "    # split tensor by number of heads\n",
    "        q, k, v = [   lin(x).view(samples, -1, self.n_head, self.d_k).transpose(1, 2)\n",
    "    # [512,512] => [512,1,8,64] => [512,8,1,64] now we have 8 heads, \n",
    "    #length 1 since conv of size 1, dim of 64 for each q,k,v, \n",
    "    #ready for input to attention [batch_size, head, length, d_tensor]\n",
    "            for lin, x in zip(self.linears, (q, k, v)) \n",
    "    # we only used 3 first linear layers since zip would \n",
    "        ]\n",
    "        \n",
    "    # calculate the attention score \n",
    "        x, self.attn = attention(q, k, v)\n",
    "\n",
    "    # concat by view func [512, 8, 1, 64] => [512,1,512] add it back to 512\n",
    "        x = (x.transpose(1, 2).contiguous().view(samples, -1, self.n_head * self.d_k))\n",
    "    # now apply the final linear layer copy\n",
    "        return self.linears[-1](x) \n",
    "   \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,n_head,d_model,hidden):\n",
    "        super(Encoder_layer, self).__init__()\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "        self.attention_layer= MultiHeadAttention(d_model, n_head)\n",
    "        self.feed_forward_layer= FeedForwardLayer(d_model, hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we make a copy for later residue adding\n",
    "        _x = x\n",
    "        # use multi-head attention we defined in part 1\n",
    "        atten = self.attention_layer(x)\n",
    "        # add residue and normalize layer\n",
    "        _atten = _x + self.norm(atten)\n",
    "        # feed forward layer which we will define later \n",
    "        x = self.feed_forward_layer(x)\n",
    "        return self.norm(x)+_atten\n",
    "\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, hidden):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, hidden, n_head, n_copy):\n",
    "        super().__init__()\n",
    "        # n_copy = 6 \n",
    "        self.layers = clones(EncoderLayer(d_model,hidden,n_head), n_copy)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # init as 515x512 matrix to make adding pos with input possible\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        # produce 0 to 511 pos index \n",
    "        pos = torch.arange(0, max_len)\n",
    "        # convert to 512x1 size\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        # pick 0,2,4...etc 256 even numbers, \n",
    "        # _2i refers to the index in above formula\n",
    "        _2i = torch.arange(0, d_model, step=2).float()\n",
    "        # pos index (512,1) divide by _2i (256)\n",
    "        # broadcasting to (512,256), so every even column apply sin func\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        # odd column go through cos func\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size() \n",
    "        #now to apply encoding\n",
    "        return self.encoding[:seq_len, :]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19aed417-537f-4149-89e0-dd937587ef64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (embedding): Embedding(10000, 256)\n",
      "  (transformer_layers): ModuleList(\n",
      "    (0-5): 6 x TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=10000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_heads, num_layers):\n",
    "        super(GPTModel, self).__init__()\n",
    "\n",
    "        # Token embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = self.create_positional_encoding(embedding_dim)\n",
    "\n",
    "        # Transformer layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Fully connected layer for prediction\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Token embedding\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Add positional encoding\n",
    "        positional_encoded = embedded + self.positional_encoding[:embedded.size(0), :]\n",
    "\n",
    "        # Transformer layers\n",
    "        transformer_output = positional_encoded\n",
    "        for layer in self.transformer_layers:\n",
    "            transformer_output = layer(transformer_output)\n",
    "\n",
    "        # Fully connected layer for prediction\n",
    "        output = self.fc(transformer_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def create_positional_encoding(self, d_model, max_len=512):\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        positional_encoding = torch.zeros((max_len, d_model))\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return positional_encoding\n",
    "\n",
    "# Example usage:\n",
    "vocab_size = 10000  # replace with your vocabulary size\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "\n",
    "model = GPTModel(vocab_size, embedding_dim, hidden_dim, num_heads, num_layers)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "913e9f3d-e4ae-4664-bdb5-a6fb3d5682c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (64) to match target batch_size (999).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m output_logits \u001b[38;5;241m=\u001b[39m model(input_tensor)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (64) to match target batch_size (999)."
     ]
    }
   ],
   "source": [
    "\n",
    "training_data = [\"This is an example sentence.\", \"Another example here.\"]\n",
    "training_data = text[:1000]\n",
    "\n",
    "# Tokenize the dataset\n",
    "# tokenized_data = encoded_data\n",
    "\n",
    "tokenized_data = tokenizer.encode_combined(training_data) \n",
    "\n",
    "# Create an instance of the GPT model\n",
    "model = GPTModel(vocab_size, embedding_dim, hidden_dim, num_heads, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for batch in dataloader:\n",
    "\n",
    "        input_tensor = batch\n",
    "        # Forward pass\n",
    "        output_logits = model(input_tensor)\n",
    "    \n",
    "        # Calculate loss\n",
    "        loss = criterion(output_logits.view(-1, vocab_size), target_tensor)\n",
    "    \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877ca51-cfa3-41ca-9041-99d40ee8e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0fcd2b-3e77-4dcf-8147-f4574cc656c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference after training\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Generate text from a seed input\n",
    "    seed_text = ''\n",
    "    seed_tokenized = tokenizer.encode_combined(seed_text)\n",
    "    input_tensor = torch.tensor(seed_tokenized).unsqueeze(0)\n",
    "    generated_indices = torch.argmax(model(input_tensor), dim=-1).squeeze().tolist()\n",
    "    generated_text = tokenizer.decode_combined(generated_indices) \n",
    "\n",
    "    print(\"Generated Text:\", ''.join(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a1491-ad9b-4dd2-8519-4c9dfb1ce480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(SimpleLanguageModel, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "        # LSTM layers\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        # Output layer\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "# Example usage:\n",
    "# Set your vocabulary size, embedding dimension, hidden dimension, and number of LSTM layers\n",
    "vocab_size = 100  # replace with the actual size of your vocabulary\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Create an instance of the SimpleLanguageModel\n",
    "model = SimpleLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08e24a61-88ef-458a-b63f-ee9821484618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 ['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']\n",
      "O'Neal\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    with open(filename, encoding='utf-8') as some_file:\n",
    "        return [unicodeToAscii(line.strip()) for line in some_file]\n",
    "\n",
    "# Build the category_lines dictionary, a list of lines per category\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "\n",
    "if n_categories == 0:\n",
    "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
    "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
    "        'the current directory.')\n",
    "\n",
    "print('# categories:', n_categories, all_categories)\n",
    "print(unicodeToAscii(\"O'Néàl\"))\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, category, input, hidden):\n",
    "        input_combined = torch.cat((category, input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(input_combined)\n",
    "        output_combined = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "import random\n",
    "\n",
    "# Random item from a list\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "# Get a random category and random line from that category\n",
    "def randomTrainingPair():\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    return category, line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2f3d4-8c25-4d5d-9487-5d505492c614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
