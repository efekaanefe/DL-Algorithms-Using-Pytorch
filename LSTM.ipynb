{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be889ea-8a2c-471c-8089-26494afcd02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "          if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff979cb9-8a0d-403f-b9e6-113f930f839f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-param\n",
    "text_file = \"tiny-shakespeare.txt\"\n",
    "with open(text_file, \"r\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be475b4-b37e-4d97-a37f-a50b6a2da559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Proted Stajyer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_device.py:77: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create a character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "# Create mappings from characters to indices and vice versa\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "# Encode the entire text as integers\n",
    "encoded_text = np.array([char_to_idx[char] for char in text])\n",
    "\n",
    "# Define the sequence length\n",
    "seq_length = 100\n",
    "num_samples = len(encoded_text) // seq_length\n",
    "\n",
    "# Create input and target sequences\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    start_idx = i * seq_length\n",
    "    end_idx = start_idx + seq_length\n",
    "    input_sequences.append(encoded_text[start_idx:end_idx])\n",
    "    target_sequences.append(encoded_text[start_idx + 1:end_idx + 1])\n",
    "\n",
    "# Convert sequences to PyTorch tensors\n",
    "input_sequences = torch.tensor(input_sequences, dtype=torch.long)\n",
    "target_sequences = torch.tensor(target_sequences, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc6591f-9953-4200-a5a7-3c1ec25c682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out.reshape(out.size(0) * out.size(1), out.size(2)))\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new_zeros(num_layers, batch_size, hidden_size),\n",
    "                weight.new_zeros(num_layers, batch_size, hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ff5f1f1-af4e-4def-82ae-956575c2e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_epochs = 20\n",
    "learning_rate = 0.002\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMModel(vocab_size, embed_size, hidden_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbdb00dc-defd-415b-a197-b7005fe6c649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.3668\n",
      "Epoch [2/20], Loss: 1.8128\n",
      "Epoch [3/20], Loss: 1.6498\n",
      "Epoch [4/20], Loss: 1.5539\n",
      "Epoch [5/20], Loss: 1.4903\n",
      "Epoch [6/20], Loss: 1.4469\n",
      "Epoch [7/20], Loss: 1.4147\n",
      "Epoch [8/20], Loss: 1.3892\n",
      "Epoch [9/20], Loss: 1.3679\n",
      "Epoch [10/20], Loss: 1.3492\n",
      "Epoch [11/20], Loss: 1.3332\n",
      "Epoch [12/20], Loss: 1.3186\n",
      "Epoch [13/20], Loss: 1.3052\n",
      "Epoch [14/20], Loss: 1.2926\n",
      "Epoch [15/20], Loss: 1.2814\n",
      "Epoch [16/20], Loss: 1.2713\n",
      "Epoch [17/20], Loss: 1.2613\n",
      "Epoch [18/20], Loss: 1.2524\n",
      "Epoch [19/20], Loss: 1.2445\n",
      "Epoch [20/20], Loss: 1.2382\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, input_sequences.size(0) - batch_size, batch_size):\n",
    "        inputs = input_sequences[i:i+batch_size]\n",
    "        targets = target_sequences[i:i+batch_size]\n",
    "\n",
    "        # Detach hidden state to prevent backpropagation through entire training history\n",
    "        hidden = tuple([h.detach() for h in hidden])\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, targets.view(-1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (input_sequences.size(0) // batch_size)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89eb8bf9-01e3-4198-8d5a-5a18b70a7b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: With thee?\n",
      "I am good that, here wherein.\n",
      "\n",
      "PETRUCHIO:\n",
      "He\n",
      "has no rough a curst master-grief,\n",
      "For sworn, to attled date. Thyself and a\n",
      "madems aside.\n",
      "\n",
      "First Sillant:\n",
      "Ashout! Was thou didst mad.\n",
      "Believe me, sir.\n",
      "\n",
      "BAPTISTA:\n",
      "To-mords, to\n",
      "speak; and what is a palms\n",
      "Upon him being sorrows be guilty.\n",
      "\n",
      "BIONDEL\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_str, char_to_idx, idx_to_char, num_generate=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_eval = torch.tensor([char_to_idx[c] for c in start_str], dtype=torch.long).unsqueeze(0)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    generated_text = start_str\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_generate):\n",
    "            output, hidden = model(input_eval, hidden)\n",
    "            output = output / temperature\n",
    "            predicted_idx = torch.multinomial(torch.softmax(output[-1], dim=0), num_samples=1).item()\n",
    "            input_eval = torch.tensor([[predicted_idx]], dtype=torch.long)\n",
    "            generated_text += idx_to_char[predicted_idx]\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "start_str = \"ROMEO: \"\n",
    "generated_text = generate_text(model, start_str, char_to_idx, idx_to_char, num_generate=300)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
