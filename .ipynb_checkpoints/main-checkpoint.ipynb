{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67dd4d36-609b-4e14-ba93-ebf08bdb15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Props to this sensei\n",
    "# https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a77eb7d-2891-46db-9649-70dbf52dd5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm # progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a483cfa-c20f-460b-b51c-7f2c8275853d",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89ca760-851f-4cfc-b0f4-f6837de852cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "text_file = \"tiny-shakespeare.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24e153-111f-490b-aa5f-9579bf99d022",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5123bdbe-4f29-476b-b2d3-2d142e5e7661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read file\n",
    "with open(text_file, \"r\") as f:\n",
    "    text = f.read()\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc14e1f1-3f5e-4e98-b838-cbbcb10b6585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the characters in the text: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Length of the characters: 65\n"
     ]
    }
   ],
   "source": [
    "char_list = sorted(list(set(text)))\n",
    "char_size = len(char_list)\n",
    "print(f\"All the characters in the text: {''.join(char_list)}\")\n",
    "print(f\"Length of the characters: {char_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fe3542-8123-49f9-8ea7-2e070befb718",
   "metadata": {},
   "source": [
    "## Tokenizer (character based, index/ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7adea02c-1c7f-4b68-8013-2c5a5666eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        self.char_to_index = None\n",
    "        self.index_to_char = None\n",
    "\n",
    "    def fit(self, char_list):  \n",
    "        self.char_to_index = {char: idx for idx, char in enumerate(char_list)}\n",
    "        self.index_to_char = {idx: char for char, idx in self.char_to_index.items()}\n",
    "\n",
    "    def encode_index(self, input_str):\n",
    "        return [self.char_to_index[char] for char in input_str]\n",
    "\n",
    "    def decode_index(self, encoded_list):\n",
    "        return ''.join([self.index_to_char[idx] for idx in encoded_list])\n",
    "\n",
    "    @staticmethod\n",
    "    def ascii_tokenizer(char):\n",
    "        return ord(char)\n",
    "\n",
    "    @staticmethod\n",
    "    def ascii_decoder(ascii_value):\n",
    "        return chr(ascii_value)\n",
    "\n",
    "    def encode_combined(self, input_str, use_ascii=False):\n",
    "        if use_ascii:\n",
    "            return [self.ascii_tokenizer(char) for char in input_str]\n",
    "        else:\n",
    "            return self.encode_index(input_str)\n",
    "\n",
    "    def decode_combined(self, encoded_list, use_ascii=False):\n",
    "        if use_ascii:\n",
    "            return ''.join([self.ascii_decoder(ascii_value) for ascii_value in encoded_list])\n",
    "        else:\n",
    "            return self.decode_index(encoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c755ae-b836-4c4d-982c-201b91bc9c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String: Hello there\n",
      "Encoded List (ASCII): [72, 101, 108, 108, 111, 32, 116, 104, 101, 114, 101]\n",
      "Decoded String (ASCII): Hello there\n",
      "Encoded List (Index): [72, 101, 108, 108, 111, 32, 116, 104, 101, 114, 101]\n",
      "Decoded String (Index): Hello there\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "tokenizer = MyTokenizer()\n",
    "tokenizer.fit(char_list)\n",
    "\n",
    "input_str = \"Hello there\"\n",
    "encoded_list_ascii = tokenizer.encode_combined(input_str, use_ascii=True)\n",
    "decoded_str_ascii = tokenizer.decode_combined(encoded_list_ascii, use_ascii=True)\n",
    "\n",
    "encoded_list_index = tokenizer.encode_combined(input_str, use_ascii=True)\n",
    "decoded_str_index = tokenizer.decode_combined(encoded_list_index, use_ascii=True)\n",
    "\n",
    "print(\"Original String:\", input_str)\n",
    "print(\"Encoded List (ASCII):\", encoded_list_ascii)\n",
    "print(\"Decoded String (ASCII):\", decoded_str_ascii)\n",
    "\n",
    "print(\"Encoded List (Index):\", encoded_list_index)\n",
    "print(\"Decoded String (Index):\", decoded_str_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3773c490-c37e-453a-8057-9fa02db53bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode all the data \n",
    "encoded_data = tokenizer.encode_combined(text) \n",
    "encoded_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43202c3-4efd-44f0-99a8-8663cdbc3aa6",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78d1d7c7-1c9c-4485-8bec-03ac33b217fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encoded_data)\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897f99c6-677a-44c7-8b16-e51881ab0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50,  1, 54, 39, 56, 14,  1, 30,  1,  1, 53, 31, 58, 53, 57,  1, 61,  6,\n",
      "        10, 56, 26, 50, 59,  1,  1, 56, 61, 57,  1, 53, 57, 56, 43, 43, 39, 21,\n",
      "        54,  8,  1, 47, 49, 41, 58, 57,  2, 16, 46, 57, 23, 47,  1, 58, 63, 53,\n",
      "        47, 40, 52, 53,  6,  6, 57, 46, 54, 51])\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, encoded_data):\n",
    "        self.encoded_data = encoded_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_data[idx]\n",
    "        \n",
    "dataset = MyDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# printing the first batch\n",
    "for batch in dataloader: \n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3534c75-0447-4a1f-98f1-e00bf209d784",
   "metadata": {},
   "source": [
    "## GPT and language models\n",
    "https://github.com/iVishalr/GPT/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a9954e-c3f3-46ed-9ed4-ef5b5716a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@mingzehe/implement-transformer-via-pytorch-step-by-step-part-2-69f020d580c6\n",
    "\n",
    "#attention \n",
    "def attention(k,q,v):\n",
    "    # q dim [batch_size,n_heads,length,d_tensor]\n",
    "    d_tensor = q.size(-1) \n",
    "    # assume dim of query/key/value vector should be same \n",
    "    # and it should be to make below calculation happen      \n",
    "    k_t = k.transpose(-2,-1) #[batch_size,n_heads,d_tensor,length]\n",
    "    score = (q @ k_t)/math.sqrt(d_tensor)\n",
    "    v= torch.softmax(score,dim=-1) @ v\n",
    "    return v,score\n",
    "\n",
    "import copy\n",
    "def clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "  # reduced dim for each Q,K,V, but added up to d_model\n",
    "        self.d_k = d_model // n_head \n",
    "        self.n_head = n_head\n",
    "        self.attn = None\n",
    "  # use the attention class defined above\n",
    "        self.attention = attention() \n",
    "\n",
    "  # 3 for K,Q,V, the forth layer is on the top for final attention score\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4) \n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        samples = q.size(0) #q init as 512x512\n",
    "    # split tensor by number of heads\n",
    "        q, k, v = [   lin(x).view(samples, -1, self.n_head, self.d_k).transpose(1, 2)\n",
    "    # [512,512] => [512,1,8,64] => [512,8,1,64] now we have 8 heads, \n",
    "    #length 1 since conv of size 1, dim of 64 for each q,k,v, \n",
    "    #ready for input to attention [batch_size, head, length, d_tensor]\n",
    "            for lin, x in zip(self.linears, (q, k, v)) \n",
    "    # we only used 3 first linear layers since zip would \n",
    "        ]\n",
    "        \n",
    "    # calculate the attention score \n",
    "        x, self.attn = attention(q, k, v)\n",
    "\n",
    "    # concat by view func [512, 8, 1, 64] => [512,1,512] add it back to 512\n",
    "        x = (x.transpose(1, 2).contiguous().view(samples, -1, self.n_head * self.d_k))\n",
    "    # now apply the final linear layer copy\n",
    "        return self.linears[-1](x) \n",
    "   \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,n_head,d_model,hidden):\n",
    "        super(Encoder_layer, self).__init__()\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "        self.attention_layer= MultiHeadAttention(d_model, n_head)\n",
    "        self.feed_forward_layer= FeedForwardLayer(d_model, hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we make a copy for later residue adding\n",
    "        _x = x\n",
    "        # use multi-head attention we defined in part 1\n",
    "        atten = self.attention_layer(x)\n",
    "        # add residue and normalize layer\n",
    "        _atten = _x + self.norm(atten)\n",
    "        # feed forward layer which we will define later \n",
    "        x = self.feed_forward_layer(x)\n",
    "        return self.norm(x)+_atten\n",
    "\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, hidden):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, hidden, n_head, n_copy):\n",
    "        super().__init__()\n",
    "        # n_copy = 6 \n",
    "        self.layers = clones(EncoderLayer(d_model,hidden,n_head), n_copy)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # init as 515x512 matrix to make adding pos with input possible\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        # produce 0 to 511 pos index \n",
    "        pos = torch.arange(0, max_len)\n",
    "        # convert to 512x1 size\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        # pick 0,2,4...etc 256 even numbers, \n",
    "        # _2i refers to the index in above formula\n",
    "        _2i = torch.arange(0, d_model, step=2).float()\n",
    "        # pos index (512,1) divide by _2i (256)\n",
    "        # broadcasting to (512,256), so every even column apply sin func\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        # odd column go through cos func\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size() \n",
    "        #now to apply encoding\n",
    "        return self.encoding[:seq_len, :]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19aed417-537f-4149-89e0-dd937587ef64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (embedding): Embedding(10000, 256)\n",
      "  (transformer_layers): ModuleList(\n",
      "    (0-5): 6 x TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=10000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_heads, num_layers):\n",
    "        super(GPTModel, self).__init__()\n",
    "\n",
    "        # Token embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = self.create_positional_encoding(embedding_dim)\n",
    "\n",
    "        # Transformer layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Fully connected layer for prediction\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Token embedding\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Add positional encoding\n",
    "        positional_encoded = embedded + self.positional_encoding[:embedded.size(0), :]\n",
    "\n",
    "        # Transformer layers\n",
    "        transformer_output = positional_encoded\n",
    "        for layer in self.transformer_layers:\n",
    "            transformer_output = layer(transformer_output)\n",
    "\n",
    "        # Fully connected layer for prediction\n",
    "        output = self.fc(transformer_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def create_positional_encoding(self, d_model, max_len=512):\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        positional_encoding = torch.zeros((max_len, d_model))\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return positional_encoding\n",
    "\n",
    "# Example usage:\n",
    "vocab_size = 10000  # replace with your vocabulary size\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "\n",
    "model = GPTModel(vocab_size, embedding_dim, hidden_dim, num_heads, num_layers)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "913e9f3d-e4ae-4664-bdb5-a6fb3d5682c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 9.4068\n",
      "Epoch 1, Loss: 0.0000\n",
      "Epoch [2/10], Loss: 7.5305\n",
      "Epoch 2, Loss: 0.0000\n",
      "Epoch [3/10], Loss: 6.8457\n",
      "Epoch 3, Loss: 0.0000\n",
      "Epoch [4/10], Loss: 6.4180\n",
      "Epoch 4, Loss: 0.0000\n",
      "Epoch [5/10], Loss: 5.9814\n",
      "Epoch 5, Loss: 0.0000\n",
      "Epoch [6/10], Loss: 5.5259\n",
      "Epoch 6, Loss: 0.0000\n",
      "Epoch [7/10], Loss: 5.1399\n",
      "Epoch 7, Loss: 0.0000\n",
      "Epoch [8/10], Loss: 4.7227\n",
      "Epoch 8, Loss: 0.0000\n",
      "Epoch [9/10], Loss: 4.3529\n",
      "Epoch 9, Loss: 0.0000\n",
      "Epoch [10/10], Loss: 4.0297\n",
      "Epoch 10, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_data = [\"This is an example sentence.\", \"Another example here.\"]\n",
    "training_data = text[:1000]\n",
    "\n",
    "# Tokenize the dataset\n",
    "# tokenized_data = encoded_data\n",
    "\n",
    "tokenized_data = tokenizer.encode_combined(training_data) \n",
    "\n",
    "# Create an instance of the GPT model\n",
    "model = GPTModel(vocab_size, embedding_dim, hidden_dim, num_heads, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Convert tokenized_text to a PyTorch tensor\n",
    "    input_tensor = torch.tensor(tokenized_data[:-1]).unsqueeze(0)  # Input sequence (excluding the last token)\n",
    "    target_tensor = torch.tensor(tokenized_data[1:])  # Target sequence (excluding the first token)\n",
    "\n",
    "    # Forward pass\n",
    "    output_logits = model(input_tensor)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(output_logits.view(-1, vocab_size), target_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    average_loss = total_loss / len(tokenized_data)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {average_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4877ca51-cfa3-41ca-9041-99d40ee8e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d0fcd2b-3e77-4dcf-8147-f4574cc656c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: \n",
      "tr  r  \n"
     ]
    }
   ],
   "source": [
    "# Inference after training\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Generate text from a seed input\n",
    "    seed_text = ''\n",
    "    seed_tokenized = tokenizer.encode_combined(seed_text)\n",
    "    input_tensor = torch.tensor(seed_tokenized).unsqueeze(0)\n",
    "    generated_indices = torch.argmax(model(input_tensor), dim=-1).squeeze().tolist()\n",
    "    generated_text = tokenizer.decode_combined(generated_indices) \n",
    "\n",
    "    print(\"Generated Text:\", ''.join(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a1491-ad9b-4dd2-8519-4c9dfb1ce480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(SimpleLanguageModel, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "        # LSTM layers\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        # Output layer\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "# Example usage:\n",
    "# Set your vocabulary size, embedding dimension, hidden dimension, and number of LSTM layers\n",
    "vocab_size = 100  # replace with the actual size of your vocabulary\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Create an instance of the SimpleLanguageModel\n",
    "model = SimpleLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2f3d4-8c25-4d5d-9487-5d505492c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "          if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
