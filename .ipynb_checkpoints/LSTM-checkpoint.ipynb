{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be889ea-8a2c-471c-8089-26494afcd02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "          if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff979cb9-8a0d-403f-b9e6-113f930f839f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115393"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = \"tiny-shakespeare.txt\"\n",
    "with open(text_file, \"r\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be475b4-b37e-4d97-a37f-a50b6a2da559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Proted Stajyer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_device.py:77: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create a character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "\n",
    "# Create mappings from characters to indices and vice versa\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "encoded_text = np.array([char_to_idx[char] for char in text])\n",
    "\n",
    "seq_length = 100 # Sequence length\n",
    "num_samples = len(encoded_text) // seq_length\n",
    "\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    start_idx = i * seq_length\n",
    "    end_idx = start_idx + seq_length\n",
    "    input_sequences.append(encoded_text[start_idx:end_idx])\n",
    "    target_sequences.append(encoded_text[start_idx + 1:end_idx + 1])\n",
    "\n",
    "input_sequences = torch.tensor(input_sequences, dtype=torch.long)\n",
    "target_sequences = torch.tensor(target_sequences, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc6591f-9953-4200-a5a7-3c1ec25c682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out.reshape(out.size(0) * out.size(1), out.size(2)))\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new_zeros(num_layers, batch_size, hidden_size),\n",
    "                weight.new_zeros(num_layers, batch_size, hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff5f1f1-af4e-4def-82ae-956575c2e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 512\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "model = LSTMModel(vocab_size, embed_size, hidden_size, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbdb00dc-defd-415b-a197-b7005fe6c649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2.4487\n",
      "Epoch [2/50], Loss: 1.9235\n",
      "Epoch [3/50], Loss: 1.7700\n",
      "Epoch [4/50], Loss: 1.6711\n",
      "Epoch [5/50], Loss: 1.6023\n",
      "Epoch [6/50], Loss: 1.5535\n",
      "Epoch [7/50], Loss: 1.5163\n",
      "Epoch [8/50], Loss: 1.4871\n",
      "Epoch [9/50], Loss: 1.4637\n",
      "Epoch [10/50], Loss: 1.4440\n",
      "Epoch [11/50], Loss: 1.4272\n",
      "Epoch [12/50], Loss: 1.4118\n",
      "Epoch [13/50], Loss: 1.3982\n",
      "Epoch [14/50], Loss: 1.3865\n",
      "Epoch [15/50], Loss: 1.3752\n",
      "Epoch [16/50], Loss: 1.3652\n",
      "Epoch [17/50], Loss: 1.3553\n",
      "Epoch [18/50], Loss: 1.3466\n",
      "Epoch [19/50], Loss: 1.3381\n",
      "Epoch [20/50], Loss: 1.3300\n",
      "Epoch [21/50], Loss: 1.3220\n",
      "Epoch [22/50], Loss: 1.3144\n",
      "Epoch [23/50], Loss: 1.3071\n",
      "Epoch [24/50], Loss: 1.3003\n",
      "Epoch [25/50], Loss: 1.2935\n",
      "Epoch [26/50], Loss: 1.2871\n",
      "Epoch [27/50], Loss: 1.2807\n",
      "Epoch [28/50], Loss: 1.2747\n",
      "Epoch [29/50], Loss: 1.2686\n",
      "Epoch [30/50], Loss: 1.2627\n",
      "Epoch [31/50], Loss: 1.2571\n",
      "Epoch [32/50], Loss: 1.2515\n",
      "Epoch [33/50], Loss: 1.2464\n",
      "Epoch [34/50], Loss: 1.2417\n",
      "Epoch [35/50], Loss: 1.2373\n",
      "Epoch [36/50], Loss: 1.2328\n",
      "Epoch [37/50], Loss: 1.2286\n",
      "Epoch [38/50], Loss: 1.2242\n",
      "Epoch [39/50], Loss: 1.2191\n",
      "Epoch [40/50], Loss: 1.2140\n",
      "Epoch [41/50], Loss: 1.2095\n",
      "Epoch [42/50], Loss: 1.2053\n",
      "Epoch [43/50], Loss: 1.2011\n",
      "Epoch [44/50], Loss: 1.1974\n",
      "Epoch [45/50], Loss: 1.1946\n",
      "Epoch [46/50], Loss: 1.1924\n",
      "Epoch [47/50], Loss: 1.1900\n",
      "Epoch [48/50], Loss: 1.1859\n",
      "Epoch [49/50], Loss: 1.1810\n",
      "Epoch [50/50], Loss: 1.1759\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, input_sequences.size(0) - batch_size, batch_size):\n",
    "        inputs = input_sequences[i:i+batch_size]\n",
    "        targets = target_sequences[i:i+batch_size]\n",
    "\n",
    "        hidden = tuple([h.detach() for h in hidden])\n",
    "        \n",
    "        # Forward \n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs, targets.view(-1))\n",
    "\n",
    "        # Backward \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (input_sequences.size(0) // batch_size)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89eb8bf9-01e3-4198-8d5a-5a18b70a7b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Juliet;\n",
      "Ould this held: it is not saddle.\n",
      "\n",
      "Gaolent me, if I be\n",
      "not mistake,\n",
      "Pardon me, Signior Baptista;\n",
      "But shall hear the gods as you any enemy,\n",
      "That she had ask you what I shout her reason and all\n",
      "From whom, madam: Margaret's faired one:\n",
      "I will not be so, no write I will conscrawns\n",
      "The tire me fr\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_str, char_to_idx, idx_to_char, num_generate=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_eval = torch.tensor([char_to_idx[c] for c in start_str], dtype=torch.long).unsqueeze(0)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    generated_text = start_str\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_generate):\n",
    "            output, hidden = model(input_eval, hidden)\n",
    "            output = output / temperature\n",
    "            predicted_idx = torch.multinomial(torch.softmax(output[-1], dim=0), num_samples=1).item()\n",
    "            input_eval = torch.tensor([[predicted_idx]], dtype=torch.long)\n",
    "            generated_text += idx_to_char[predicted_idx]\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "start_str = \"ROMEO: \"\n",
    "generated_text = generate_text(model, start_str, char_to_idx, idx_to_char, num_generate=300)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
